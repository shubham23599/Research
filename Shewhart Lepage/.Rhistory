v <- rnorm(n)
wrs_ps <- WRS(u,v)-(n*(m+n+1))/2
wrs_ns <- WRS(-u,-v)-(n*(m+n+1))/2
print(c(wrs_ps,wrs_ns))
if(round(wrs_ps,6) == -round(wrs_ns,6)) wrs <- wrs+1
vw_ps <- VW(u,v)
VW_ns <- VW(-u,-v)
if(round(vw_ps,6) == round(-VW_ns,6)) vw <- vw+1
}
wrs/N
vw/N
# Define the test statistic functions
WRS = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(rep(c(0, 1), c(m, n)) * rank(c(u, v)))
S
}
MW = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(sapply(1:m, function(i) u[i] > v))
S
}
VW = function(u, v) {
m = length(u); n = length(v); N = m + n
Ind = rep(c(0, 1), c(m, n))
XN = sum(Ind * qnorm(rank(c(u, v)) / (N + 1)))
XN
}
Mood = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(((rank(c(u, v))) - 1/2 * (N + 1))^2 * rep(c(0, 1), c(m, n)))
S
}
AB = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(abs((rank(c(u, v))) - 1/2 * (N + 1)) * rep(c(0, 1), c(m, n)))
S
}
m <- 100
n <- 5
wrs <- 0
vw <- 0
ab <- 0
m < 0
N <- 1000
for(i in 1:N){
u <- rnorm(m)
v <- rnorm(n)
wrs_ps <- WRS(u,v)-(n*(m+n+1))/2
wrs_ns <- WRS(-u,-v)-(n*(m+n+1))/2
print(c(wrs_ps,wrs_ns))
if(round(wrs_ps,6) == -round(wrs_ns,6)) wrs <- wrs+1
vw_ps <- VW(u,v)
VW_ns <- VW(-u,-v)
if(round(vw_ps,6) == round(-VW_ns,6)) vw <- vw+1
}
wrs/N
vw/N
# Define the test statistic functions
WRS = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(rep(c(0, 1), c(m, n)) * rank(c(u, v)))
S
}
MW = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(sapply(1:m, function(i) u[i] > v))
S
}
VW = function(u, v) {
m = length(u); n = length(v); N = m + n
Ind = rep(c(0, 1), c(m, n))
XN = sum(Ind * qnorm(rank(c(u, v)) / (N + 1)))
XN
}
Mood = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(((rank(c(u, v))) - 1/2 * (N + 1))^2 * rep(c(0, 1), c(m, n)))
S
}
AB = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(abs((rank(c(u, v))) - 1/2 * (N + 1)) * rep(c(0, 1), c(m, n)))
S
}
corr_matrix_scatter <- function(simulation = 1000, location = 0, scale = 1) {
library(ggplot2)
library(dplyr)
library(tidyr)
library(officer)
library(flextable)
library(GGally)
library(progress)
m_values <- c(50, 75, 100, 125, 150, 175, 200, 300)
n_values <- c(5, 10, 15, 20, 25, 30, 50)
# Initialize Word document
doc <- read_docx()
doc <- doc %>%
body_add_par("Introduction", style = "heading 1") %>%
body_add_par(paste(
"This document shows the correlation matrix, p-value matrix, and scatter plot matrix for different combinations of m and n based on",
simulation,
"simulations, assuming the process is out of control (OOC), i.e., under the alternative hypothesis (Ha), with location (",
location,
") and scale (",
scale,
") while IC location and scale parameters are (0,1). Here, m denotes the reference sample size, and n denotes the testing or monitoring sample size. The statistics calculated are:",
sep = " "
), style = "Normal") %>%
body_add_par("1. WRS - Wilcoxon Rank Sum Test", style = "Normal") %>%
body_add_par("2. MW - Mann-Whitney Test", style = "Normal") %>%
body_add_par("3. VW - Vander Waerden Test", style = "Normal") %>%
body_add_par("4. AB - Ansari-Bradley Test", style = "Normal") %>%
body_add_par("5. Mood - Mood's Median Test", style = "Normal") %>%
body_add_par("For simulation, I used the normal distribution due to the nonparametric setup. In Lepage (1971), it was proven that under the null hypothesis (H0), the WRS and AB statistics are uncorrelated for all m and n.", style = "Normal") %>%
body_add_par("Reference: Lepage, Y. (1971). A combination of Wilcoxon's and Ansari-Bradley's statistics. Biometrika, 58(1), 213-217.", style = "Normal") %>%
body_add_par("This document also provides correlation matrices and scatter plot matrices to analyze the relationship between different statistical tests.", style = "Normal") %>%
body_add_par("The following sections detail the results for each combination of m and n, as well as the location and scale parameters.", style = "Normal")
# Add Index Page
doc <- doc %>%
body_add_par("Index", style = "heading 1") %>%
body_add_par("This document contains results for the following combinations of m, n, location, and scale:", style = "Normal")
# Create an index list for combinations of m, n, location, and scale
index_list <- list()
for (m in m_values) {
for (n in n_values) {
index_list <- c(index_list, paste("Combination of m =", m, ", n =", n, ", location =", location, ", scale =", scale))
}
}
# Add the index list to the document
for (entry in index_list) {
doc <- doc %>%
body_add_par(entry, style = "Normal")
}
# Initialize progress bar
total_combinations <- length(m_values) * length(n_values)
pb <- progress_bar$new(
format = "  Generating [:bar] :percent (:current/:total) ETA: :eta",
total = total_combinations,
clear = FALSE,
width = 60
)
# Add page break for results
doc <- doc %>%
body_add_par("Results", style = "heading 1") %>%
body_add_par("The following sections present the correlation matrices, p-value matrices, and scatter plot matrices for each combination of m, n, location, and scale.", style = "Normal")
for (m in m_values) {
for (n in n_values) {
pb$tick()  # Update progress bar
# Add section header
doc <- doc %>% body_add_par(paste("Results for m =", m, ", n =", n, ", location =", location, ", scale =", scale), style = "heading 2")
# Generate data
data <- data.frame(WRS = numeric(simulation),
MW = numeric(simulation),
VW = numeric(simulation),
AB = numeric(simulation),
Mood = numeric(simulation))
for (i in 1:simulation) {
u <- rnorm(m, mean = 0, sd = scale)
v <- rnorm(n, mean = location, sd = scale)
data$WRS[i] <- WRS(u, v)
data$MW[i] <- MW(u, v)
data$VW[i] <- VW(u, v)
data$AB[i] <- AB(u, v)
data$Mood[i] <- Mood(u, v)
}
# Calculate correlation and p-value matrices
correlation_pvalues <- function(data) {
n <- ncol(data)
p_mat <- matrix(NA, n, n)
colnames(p_mat) <- colnames(data)
rownames(p_mat) <- colnames(data)
for (i in 1:(n - 1)) {
for (j in (i + 1):n) {
test <- cor.test(data[[i]], data[[j]])
p_mat[i, j] <- test$p.value
p_mat[j, i] <- test$p.value
}
}
diag(p_mat) <- NA
return(p_mat)
}
corr_matrix <- cor(data)
p_matrix <- correlation_pvalues(data)
# Add correlation matrix
corr_matrix_df <- as.data.frame(round(corr_matrix, 4))
corr_matrix_df <- cbind(Statistic = rownames(corr_matrix_df), corr_matrix_df)
ft_corr <- as_flextable(corr_matrix_df) %>%
colformat_double(j = 2:ncol(corr_matrix_df), digits = 4)
doc <- doc %>% body_add_par("Correlation Matrix", style = "heading 3") %>%
body_add_flextable(ft_corr)
# Add p-value matrix
p_matrix_df <- as.data.frame(round(p_matrix, 4))
p_matrix_df <- cbind(Statistic = rownames(p_matrix_df), p_matrix_df)
ft_p <- as_flextable(p_matrix_df) %>%
colformat_double(j = 2:ncol(p_matrix_df), digits = 4)
doc <- doc %>% body_add_par("P-Value Matrix", style = "heading 3") %>%
body_add_flextable(ft_p)
# Scatterplot matrix
scatter_matrix <- ggpairs(data, title = paste("Scatterplot Matrix for m =", m, ", n =", n, ", location =", location, ", scale =", scale))
plot_file <- tempfile(fileext = ".png")
ggsave(plot_file, scatter_matrix, width = 12, height = 12)
doc <- doc %>% body_add_par("Scatter Plot Matrix:", style = "heading 3") %>%
body_add_img(src = plot_file, width = 6.69, height = 8)
unlink(plot_file) # Remove temp file
}
}
print(doc, target = "Test_Statistics_Summary_OOC.docx")
message("Word document created: Test_Statistics_Summary.docx")
}
# Example call
corr_matrix_scatter(simulation = 100000, location = 0.5, scale = 1)
# Define the test statistic functions
WRS = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(rep(c(0, 1), c(m, n)) * rank(c(u, v)))
S
}
MW = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(sapply(1:m, function(i) u[i] > v))
S
}
VW = function(u, v) {
m = length(u); n = length(v); N = m + n
Ind = rep(c(0, 1), c(m, n))
XN = sum(Ind * qnorm(rank(c(u, v)) / (N + 1)))
XN
}
Mood = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(((rank(c(u, v))) - 1/2 * (N + 1))^2 * rep(c(0, 1), c(m, n)))
S
}
AB = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(abs((rank(c(u, v))) - 1/2 * (N + 1)) * rep(c(0, 1), c(m, n)))
S
}
m <- 100
n <- 5
wrs <- 0
vw <- 0
ab <- 0
m < 0
N <- 10000
for(i in 1:N){
u <- rnorm(m)
v <- rnorm(n)
wrs_ps <- WRS(u,v)-(n*(m+n+1))/2
wrs_ns <- WRS(-u,-v)-(n*(m+n+1))/2
if(round(wrs_ps,6) == -round(wrs_ns,6)) wrs <- wrs+1
vw_ps <- VW(u,v)
VW_ns <- VW(-u,-v)
if(round(vw_ps,6) == round(-VW_ns,6)) vw <- vw+1
}
wrs/N
vw/N
# Define the test statistic functions
WRS = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(rep(c(0, 1), c(m, n)) * rank(c(u, v)))
S
}
MW = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(sapply(1:m, function(i) u[i] > v))
S
}
VW = function(u, v) {
m = length(u); n = length(v); N = m + n
Ind = rep(c(0, 1), c(m, n))
XN = sum(Ind * qnorm(rank(c(u, v)) / (N + 1)))
XN
}
Mood = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(((rank(c(u, v))) - 1/2 * (N + 1))^2 * rep(c(0, 1), c(m, n)))
S
}
AB = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(abs((rank(c(u, v))) - 1/2 * (N + 1)) * rep(c(0, 1), c(m, n)))
S
}
m <- 100
n <- 5
wrs <- 0
vw <- 0
ab <- 0
mood <- 0
N <- 10000
for(i in 1:N){
u <- rnorm(m)
v <- rnorm(n)
wrs_ps <- WRS(u,v)-(n*(m+n+1))/2
wrs_ns <- WRS(-u,-v)-(n*(m+n+1))/2
if(round(wrs_ps,8) == -round(wrs_ns,8)) wrs <- wrs+1
vw_ps <- VW(u,v)
VW_ns <- VW(-u,-v)
if(round(vw_ps,8) == round(-VW_ns,8)) vw <- vw+1
ab_ps <- AB(u,v)
ab_ns <- AB(u,v)
if(round(ab_ps,8) == round(ab_ns,8)) ab <- ab+1
mood_ps <- Mood(u,v)
mood_ns <- Mood(-u,-v)
if(round(mood_ps,8) == round(mood_ns,8)) mood <- mood+1
}
wrs/N
vw/N
ab/N
mood/N
# Define the test statistic functions
WRS = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(rep(c(0, 1), c(m, n)) * rank(c(u, v)))
S
}
MW = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(sapply(1:m, function(i) u[i] > v))
S
}
VW = function(u, v) {
m = length(u); n = length(v); N = m + n
Ind = rep(c(0, 1), c(m, n))
XN = sum(Ind * qnorm(rank(c(u, v)) / (N + 1)))
XN
}
Mood = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(((rank(c(u, v))) - 1/2 * (N + 1))^2 * rep(c(0, 1), c(m, n)))
S
}
AB = function(u, v) {
m = length(u); n = length(v); N = m + n
S = sum(abs((rank(c(u, v))) - 1/2 * (N + 1)) * rep(c(0, 1), c(m, n)))
S
}
m <- 100
n <- 5
wrs <- 0
vw <- 0
ab <- 0
mood <- 0
N <- 10000
for(i in 1:N){
u <- rlnorm(m)
v <- rlnorm(n)
wrs_ps <- WRS(u,v)-(n*(m+n+1))/2
wrs_ns <- WRS(-u,-v)-(n*(m+n+1))/2
if(round(wrs_ps,8) == -round(wrs_ns,8)) wrs <- wrs+1
vw_ps <- VW(u,v)
VW_ns <- VW(-u,-v)
if(round(vw_ps,8) == round(-VW_ns,8)) vw <- vw+1
ab_ps <- AB(u,v)
ab_ns <- AB(u,v)
if(round(ab_ps,8) == round(ab_ns,8)) ab <- ab+1
mood_ps <- Mood(u,v)
mood_ns <- Mood(-u,-v)
if(round(mood_ps,8) == round(mood_ns,8)) mood <- mood+1
}
wrs/N
vw/N
ab/N
mood/N
x = rnorm(10)
rank(x)
rank(-x)
11-rank(x)
# Load data
data("EuStockMarkets")
dax <- as.numeric(EuStockMarkets[, "DAX"])
# Select last 10 observations for example
X <- dax[1851:1860]
n <- length(X)
D <- numeric(n)
S <- numeric(n)
Z <- numeric(n)
# Initialize
D[1] <- 0
S[1] <- 0
Z[1] <- 0
Z[2] <- 0
D[2] <- (X[2] - X[1])^2
S[2] <- (1/2) * (X[2] - X[1])^2  # bar X1 = X1
# SAT formula for j >= 3
for(j in 3:n){
# Compute mean of previous j-1 observations
Xbar <- mean(X[1:(j-1)])
# Recursive components
D[j] <- D[j-1] + (X[j] - X[j-1])^2
S[j] <- S[j-1] + ((j-1)/j) * (X[j] - Xbar)^2
# SAT statistic
Z[j] <- (1 - D[j]/(2*S[j])) * sqrt((j-1)*(j+1)/(j-2))
}
# SAT stopping boundaries
a <- 0.488
b <- 2.237
# Create decision vector
Decision <- rep("Continue", n)
Decision[Z >= b] <- "Reject H0"
Decision[1:2] <- "Continue"
Decision[Z <= a & Z != 0] <- "Accept H0"
# Combine results
results <- data.frame(
Observation = 1:n,
DAX = round(X, 2),
D_j = round(D, 2),
S_j = round(S, 2),
Z_j = round(Z, 3),
Decision = Decision
)
print(results)
library(DescTools)
library(ggplot2)
source("Distance_Statistics.R")
df=read.csv("Flow width measurement data.csv")
head(df)
Phase_1 = df[df$Phase==1,paste0("X", 1:5)]
Phase_2 = df[df$Phase==2,paste0("X", 1:5)]
u = unlist(lapply(paste0("X", 1:5), function(x) Phase_1[[x]]))
print(paste0("m=",length(u)))
# Define UDF to process and update plot_df
generate_plot_df <- function(phase_2, u) {
# Initialize plot_df with default values
plot_df <- data.frame(
t = 0,
SC = 0,
WRSAB = 0,
WRSM = 0,
MWAB = 0,
MWM = 0,
VWAB = 0,
VWM = 0,
PvalMW = 0,
PvalVW = 0,
PvalAB = 0,
PvalM = 0,
PvalMN = 0
)
# Populate plot_df with values calculated for each sample
for (t in 1:nrow(phase_2)) {
v <- c(as.vector(unlist(phase_2[t,]))) #c(t,t-1)
print(length(v))
# Update plot_df with computed statistics for sample t
plot_df[t, ] <- c(
t,
SC(u, v),
WRSAB(u, v),
WRSM(u, v),
MWAB(u, v),
MWM(u, v),
VWAB(u, v),
VWM(u, v),
wilcox.test(u, v)$p.value,
VanWaerdenTest(list(v, u))$p.value,
ansari.test(u, v)$p.value,
mood.test(u, v)$p.value,
mood.test(u - median(u), v - median(v))$p.value
)
}
return(plot_df)
}
plot_df <- generate_plot_df(Phase_2, u)
print(plot_df)
# Define the plot function (UDF)
create_plot <- function(data, chart_name,lower_limit, upper_limit,stat_name) {
dataset_name <- ""
shifts <- 1:nrow(data)
ARL <- data[[chart_name]]
chart <- rep("Statistic", length(shifts))
plot_data <- data.frame(chart, shifts, ARL)
colnames(plot_data) <- c("Charts", "Shifts", "ARL")
# Generate the plot
plot <- ggplot(plot_data, aes(x = Shifts, y = ARL)) +
geom_line(color = "black", size = 1.1) +
geom_point(color = "black", size = 2.2) +
theme_bw() +
geom_hline(yintercept = upper_limit, size = 0.9) +
geom_hline(yintercept = lower_limit) +
theme(
legend.position = c(0.2, 0.9),
legend.title = element_blank(),
axis.title.y = element_text(size = 20),
axis.title.x = element_text(size = 20),
axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15),
text = element_text(size = 20),
axis.text = element_text(size = 10),
axis.ticks.length = unit(0.45, "cm")
) +
xlab("Sample Number") +
ylab(stat_name) +
scale_x_continuous(breaks = seq(0, nrow(data), by = 2)) +
scale_y_continuous(limits = c(floor(min(ARL)), ceiling(max(ARL))),
breaks = seq(floor(min(ARL)), ceiling(max(ARL)), by = 2)) +
geom_text(aes(x = nrow(data) + 1, label = "UCL", y = upper_limit - 0.8), size = 5)
return(plot)
}
# Generate and display the plot
ARL_graph <- create_plot(plot_df,"VWAB",0,11.46,bquote(L["V,A"]))
print(ARL_graph)
ARL_graph <- create_plot(plot_df,"VWM",0,12.16,bquote(L["V,M"]))
print(ARL_graph)
ARL_graph <- create_plot(plot_df,"WRSAB",0,10.8,bquote(L["W,A"]))
print(ARL_graph)
ARL_graph <- create_plot(plot_df,"SC",0,5.78,bquote('Cucconi'))
print(ARL_graph)
# Save the final data set
output_file <- "Example Flow Width measurement Statistics for n=5 .csv"
write.csv(plot_df, output_file)
